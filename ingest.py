import sqlite3
import torch
import argparse
import os  # osãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from pinecone import Pinecone, ServerlessSpec # ServerlessSpec ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from sentence_transformers import SentenceTransformer
from typing import List

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…±é€šã®è¨­å®šã¨ã‚¹ã‚­ãƒ¼ãƒã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import config
from schemas import DiaryEntry

# --- åˆæœŸåŒ– ---

# Embeddingãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
# ã“ã®å‡¦ç†ã¯é‡ã„ã®ã§ã€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã§ä¸€åº¦ã ã‘å®Ÿè¡Œã™ã‚‹
try:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Ingest: Using device '{device}' for embedding model.")
    embedding_model = SentenceTransformer(config.EMBEDDING_MODEL_NAME, device=device)
except Exception as e:
    print(f"Error loading SentenceTransformer model: {e}")
    embedding_model = None

# Pineconeã¸ã®æ¥ç¶š
try:
    # config.pyãŒ.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ã‚­ãƒ¼ã‚’æ¸¡ã™å¿…è¦ã¯ãªã„
    pc = Pinecone()
except Exception as e:
    print(f"Error connecting to Pinecone: {e}")
    pc = None

# --- DBæ“ä½œ ---

def init_pinecone_index() -> 'pinecone.Index':
    """Pineconeã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå­˜åœ¨ã—ãªã„å ´åˆã«ä½œæˆã—ã€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™"""
    if not pc:
        raise ConnectionError("Pinecone client is not initialized.")
    if config.PINECONE_INDEX_NAME not in pc.list_indexes().names():
        print(f"Creating Pinecone index '{config.PINECONE_INDEX_NAME}'...")
        # specå¼•æ•°ã‚’è¿½åŠ ã—ã¦ã€ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã™ã‚‹
        pc.create_index(
            name=config.PINECONE_INDEX_NAME,
            dimension=config.EMBEDDING_DIMENSION,
            metric="cosine",
            spec=ServerlessSpec(
                cloud="aws",
                # â–¼â–¼â–¼ã€ä¿®æ­£ç‚¹ã€‘ç„¡æ–™ãƒ—ãƒ©ãƒ³ã§åˆ©ç”¨å¯èƒ½ãªãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã«å¤‰æ›´â–¼â–¼â–¼
                region="us-east-1"
            )
        )
        # â–²â–²â–²ã€ä¿®æ­£ã“ã“ã¾ã§ã€‘â–²â–²â–²
        print("Index created successfully.")
    return pc.Index(config.PINECONE_INDEX_NAME)

def init_sqlite_db():
    """SQLiteã®ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã«ä½œæˆã™ã‚‹"""
    # DBãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªãƒ»ä½œæˆã™ã‚‹
    db_dir = os.path.dirname(config.SQLITE_DB_PATH)
    if db_dir:
        os.makedirs(db_dir, exist_ok=True)

    # æ–°ã—ã„ã‚¹ã‚­ãƒ¼ãƒã«åˆã‚ã›ã¦diary_entriesãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å®šç¾©
    con = sqlite3.connect(config.SQLITE_DB_PATH)
    cur = con.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS diary_entries (
        date TEXT PRIMARY KEY,
        body TEXT NOT NULL,
        location TEXT,
        tags TEXT
    );
    """)
    # (enrichments, generationsãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆã‚‚ã“ã“ã§è¡Œã†)
    con.commit()
    return con

def get_diary_by_date(date: str) -> dict | None:
    """æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ã®æ—¥è¨˜ã‚’SQLiteã‹ã‚‰å–å¾—ã™ã‚‹"""
    con = init_sqlite_db()
    con.row_factory = sqlite3.Row
    cursor = con.cursor()
    cursor.execute("SELECT * FROM diary_entries WHERE date = ?", (date,))
    row = cursor.fetchone()
    con.close()
    return dict(row) if row else None

# --- ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ ---

def ingest_diaries(diaries: List[DiaryEntry]):
    """
    æ—¥è¨˜ã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€SQLiteã¨Pineconeã®ä¸¡æ–¹ã«ä¿å­˜ã™ã‚‹
    ã“ã®é–¢æ•°ã¯api_server.pyã‚„ä»–ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹
    """
    if not embedding_model:
        raise RuntimeError("Embedding model is not loaded.")

    # 1. DB/Indexã®åˆæœŸåŒ–
    sqlite_con = init_sqlite_db()
    pinecone_index = init_pinecone_index()
    
    # 2. SQLiteã¸ã®ä¿å­˜
    cursor = sqlite_con.cursor()
    for entry in diaries:
        # Pydanticãƒ¢ãƒ‡ãƒ«ã‹ã‚‰è¾æ›¸ã«å¤‰æ›ã—ã¦ä¿å­˜
        tags_str = ",".join(entry.tags) if entry.tags else None
        cursor.execute(
            "INSERT OR REPLACE INTO diary_entries (date, body, location, tags) VALUES (?, ?, ?, ?)",
            (entry.date, entry.body, entry.location, tags_str)
        )
    sqlite_con.commit()
    sqlite_con.close()
    print(f"Saved {len(diaries)} entries to SQLite.")

    # 3. Pineconeã¸ã®Upsert (ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨ä¿å­˜)
    print("Embedding diary bodies for Pinecone...")
    texts_to_embed = [d.body for d in diaries]
    vectors = embedding_model.encode(texts_to_embed, show_progress_bar=True).tolist()
    
    vectors_to_upsert = []
    for i, entry in enumerate(diaries):
        vectors_to_upsert.append({
            "id": entry.date, # æ—¥ä»˜ã‚’Vector DBã®IDã¨ã™ã‚‹
            "values": vectors[i],
            "metadata": {
                "text": entry.body,
                "date": entry.date,
                "location": entry.location or ""
                # ã‚¿ã‚°ãªã©ã®ä»–ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚ã“ã“ã«è¿½åŠ å¯èƒ½
            }
        })
    
    print(f"Upserting {len(vectors_to_upsert)} vectors to Pinecone...")
    pinecone_index.upsert(vectors=vectors_to_upsert)
    print("Upsert to Pinecone complete.")

# --- CLIå®Ÿè¡Œç”¨ ---

def run_sample_ingest():
    """
    --sampleãƒ•ãƒ©ã‚°ãŒæŒ‡å®šã•ã‚ŒãŸã¨ãã«å®Ÿè¡Œã•ã‚Œã‚‹
    é–‹ç™ºã®åˆæœŸæ®µéšã§DBã«ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥ã™ã‚‹ãŸã‚ã®é–¢æ•°
    """
    print("Running sample ingestion...")
    sample_diaries = [
        DiaryEntry(date="2025-09-22", body="åˆå‰ä¸­ã«è¿‘æ‰€ã®å…¬åœ’ã‚’æ•£æ­©ã—ãŸã€‚ã‚«ãƒ•ã‚§ã§ã‚†ã£ãã‚Šã¨èª­æ›¸ã‚’æ¥½ã—ã‚“ã ã€‚", location="å¯Œå±±å¸‚", tags=["æ•£æ­©", "èª­æ›¸"]),
        DiaryEntry(date="2025-09-23", body="ä¸€æ—¥ä¸­ã€é›¨ãŒé™ã£ã¦ã„ãŸã€‚å®¶ã§æƒé™¤ã‚’ã—ãŸã‚Šã€æ–°ã—ã„ãƒ¬ã‚·ãƒ”ã§æ–™ç†ã«æŒ‘æˆ¦ã—ãŸã‚Šã—ãŸã€‚å¤œã¯æ˜ ç”»ã‚’è¦³ã¦éã”ã—ãŸã€‚", location="å¯Œå±±å¸‚", tags=["åœ¨å®…", "é›¨", "æ˜ ç”»"]),
        DiaryEntry(date="2025-09-24", body="å¤§å­¦ã§ä¸€æ—¥ä¸­ã€ç ”ç©¶ã«æ²¡é ­ã—ãŸã€‚å¤•æ–¹ã€ç–²ã‚ŒãŸé ­ã§å‹äººã¨ãƒ©ãƒ¼ãƒ¡ãƒ³ã‚’é£Ÿã¹ã«è¡Œã£ãŸã®ãŒè‰¯ã„æ¯æŠœãã«ãªã£ãŸã€‚", location="å¯Œå±±å¸‚", tags=["ç ”ç©¶", "é£Ÿäº‹"])
    ]
    try:
        ingest_diaries(sample_diaries)
        print("\nSample ingestion complete. Run 'python ingest.py --sample' again to overwrite.")
    except Exception as e:
        print(f"\nAn error occurred during sample ingestion: {e}")
        print("Please check your API keys and model configuration.")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Ingestion script for AI Diary Interpolation project.")
    parser.add_argument("--sample", action="store_true", help="Run with sample data to populate databases.")
    args = parser.parse_args()

    if args.sample:
        run_sample_ingest()
    else:
        print("This script is intended to be used with flags, or imported as a module.")
        print("To populate with sample data, run: python ingest.py --sample")




# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-
# """
# ingest.py â€” ã€Œæ—¥è¨˜ã®è£œé–“ã€PoC: ç„¡æ–™APIã§æ—¥ä»˜ãƒ»å ´æ‰€ã‹ã‚‰å¤©æ°—/æ—¥ã®å‡ºå…¥ã‚’å–å¾—ã—ä¿å­˜ + æ—¥è¨˜é–²è¦§UI
#   - Geocoding: Open-Meteo Geocoding API (no key)
#   - Historical Weather: Open-Meteo Weather API (no key)
#   - Sunrise/Sunset: sunrise-sunset.org (no key)
# ä¿å­˜å…ˆ:
#   - ./data/diary_enriched.sqlite (SQLite)

# å®Ÿè¡Œæ–¹æ³•:
#   # 1) CLIãƒ¢ãƒ¼ãƒ‰ï¼ˆå¾“æ¥ã©ãŠã‚Šï¼‰
#   python ingest.py --cli --date 2025-03-21 --place å¯Œå±±å¸‚
#   python ingest.py --cli --date 2025-03-21 --lat 36.695 --lon 137.213

#   # 2) UIãƒ¢ãƒ¼ãƒ‰ï¼ˆStreamlitï¼‰
#   python -m streamlit run ingest.py --server.port 8000 --server.address 0.0.0.0
# """
# import argparse
# import datetime as dt
# import json
# import os
# import sqlite3
# import time
# from typing import Dict, Optional, Tuple

# import requests

# DB_PATH = "./data/diary_enriched.sqlite"
# SESSION = requests.Session()
# SESSION.headers.update({"User-Agent": "DiaryInterpolationPoC/0.2 (+example.org)"})


# # -----------------------------
# # Utilities
# # -----------------------------
# def retry_get(url: str, params: Dict, tries=3, backoff=(0.5, 1, 2)) -> Optional[requests.Response]:
#     for i in range(tries):
#         try:
#             r = SESSION.get(url, params=params, timeout=15)
#             if r.status_code == 200:
#                 return r
#         except requests.RequestException:
#             pass
#         time.sleep(backoff[min(i, len(backoff) - 1)])
#     return None


# def ensure_db():
#     os.makedirs("./data", exist_ok=True)
#     con = sqlite3.connect(DB_PATH)
#     cur = con.cursor()
#     # å¤©æ°—ï¼ˆæ—¥æ¬¡ï¼‰
#     cur.execute("""
#     CREATE TABLE IF NOT EXISTS weather_daily (
#         date TEXT NOT NULL,
#         lat REAL NOT NULL,
#         lon REAL NOT NULL,
#         tmax_c REAL,
#         tmin_c REAL,
#         precip_mm REAL,
#         weather_code INTEGER,
#         weather_text TEXT,
#         source TEXT,
#         PRIMARY KEY(date, lat, lon)
#     );
#     """)
#     # æ—¥ã®å‡º/æ—¥ã®å…¥
#     cur.execute("""
#     CREATE TABLE IF NOT EXISTS sun_info (
#         date TEXT NOT NULL,
#         lat REAL NOT NULL,
#         lon REAL NOT NULL,
#         sunrise_utc TEXT,
#         sunset_utc TEXT,
#         source TEXT,
#         PRIMARY KEY(date, lat, lon)
#     );
#     """)
#     # æ—¥è¨˜æœ¬æ–‡ï¼ˆé–²è¦§ç”¨ï¼‰
#     cur.execute("""
#     CREATE TABLE IF NOT EXISTS diary_entries (
#         id INTEGER PRIMARY KEY AUTOINCREMENT,
#         date TEXT NOT NULL,          -- ä¾‹: 2025-09-24
#         title TEXT,
#         body TEXT NOT NULL,
#         location TEXT,
#         tags TEXT,
#         created_at TEXT DEFAULT (datetime('now'))
#     );
#     """)
#     con.commit()
#     con.close()


# def insert_or_replace(table: str, row: Dict):
#     con = sqlite3.connect(DB_PATH)
#     cur = con.cursor()
#     cols = ",".join(row.keys())
#     placeholders = ",".join(["?"] * len(row))
#     sql = f"INSERT OR REPLACE INTO {table} ({cols}) VALUES ({placeholders});"
#     cur.execute(sql, list(row.values()))
#     con.commit()
#     con.close()


# # -----------------------------
# # Geocoding (place -> lat/lon)
# # -----------------------------
# def geocode_place(place: str) -> Optional[Tuple[float, float, str]]:
#     r = retry_get(
#         "https://geocoding-api.open-meteo.com/v1/search",
#         {"name": place, "count": 1, "language": "ja", "format": "json"}
#     )
#     if not r:
#         return None
#     data = r.json()
#     if not data.get("results"):
#         return None
#     res = data["results"][0]
#     lat = float(res["latitude"])
#     lon = float(res["longitude"])
#     name = res.get("name") or place
#     admin1 = res.get("admin1") or ""
#     country = res.get("country") or ""
#     resolved = " ".join([x for x in [name, admin1, country] if x])
#     return lat, lon, resolved


# # -----------------------------
# # Historical Weather (daily)
# # -----------------------------
# WEATHER_CODE_MAP = {
#     0: "å¿«æ™´", 1: "æ™´ã‚Œ", 2: "è–„æ›‡ã‚Š", 3: "æ›‡ã‚Š",
#     45: "éœ§", 48: "éœ§æ°·", 51: "éœ§é›¨ï¼ˆå¼±ï¼‰", 53: "éœ§é›¨ï¼ˆä¸­ï¼‰", 55: "éœ§é›¨ï¼ˆå¼·ï¼‰",
#     61: "é›¨ï¼ˆå¼±ï¼‰", 63: "é›¨ï¼ˆä¸­ï¼‰", 65: "é›¨ï¼ˆå¼·ï¼‰",
#     71: "é›ªï¼ˆå¼±ï¼‰", 73: "é›ªï¼ˆä¸­ï¼‰", 75: "é›ªï¼ˆå¼·ï¼‰",
#     95: "é›·é›¨ï¼ˆå¼±ï¼‰", 96: "é›·é›¨ï¼ˆé›¹ã‚ã‚Šå¼±ï¼‰", 99: "é›·é›¨ï¼ˆé›¹ã‚ã‚Šå¼·ï¼‰",
# }


# def fetch_daily_weather(date_str: str, lat: float, lon: float) -> Optional[Dict]:
#     params = {
#         "latitude": lat,
#         "longitude": lon,
#         "start_date": date_str,
#         "end_date": date_str,
#         "daily": "temperature_2m_max,temperature_2m_min,precipitation_sum,weathercode",
#         "timezone": "UTC",
#     }
#     r = retry_get("https://api.open-meteo.com/v1/forecast", params)
#     if not r:
#         return None
#     js = r.json()
#     daily = js.get("daily")
#     if not daily:
#         return None
#     try:
#         tmax = daily["temperature_2m_max"][0]
#         tmin = daily["temperature_2m_min"][0]
#         precip = daily["precipitation_sum"][0]
#         code = int(daily["weathercode"][0])
#         text = WEATHER_CODE_MAP.get(code, f"å¤©æ°—ã‚³ãƒ¼ãƒ‰{code}")
#         return {
#             "tmax_c": tmax,
#             "tmin_c": tmin,
#             "precip_mm": precip,
#             "weather_code": code,
#             "weather_text": text,
#             "source": "open-meteo",
#         }
#     except Exception:
#         return None


# # -----------------------------
# # Sunrise / Sunset
# # -----------------------------
# def fetch_sunrise_sunset(date_str: str, lat: float, lon: float) -> Optional[Dict]:
#     r = retry_get(
#         "https://api.sunrise-sunset.org/json",
#         {"lat": lat, "lng": lon, "date": date_str, "formatted": 0}
#     )
#     if not r:
#         return None
#     js = r.json()
#     if js.get("status") != "OK":
#         return None
#     res = js.get("results", {})
#     return {
#         "sunrise_utc": res.get("sunrise"),
#         "sunset_utc": res.get("sunset"),
#         "source": "sunrise-sunset.org"
#     }


# # -----------------------------
# # Normalize & Persist
# # -----------------------------
# def enrich_and_store(date_str: str, lat: float, lon: float) -> Dict:
#     ensure_db()

#     weather = fetch_daily_weather(date_str, lat, lon)
#     if weather:
#         insert_or_replace("weather_daily", {
#             "date": date_str, "lat": lat, "lon": lon,
#             "tmax_c": weather["tmax_c"], "tmin_c": weather["tmin_c"],
#             "precip_mm": weather["precip_mm"],
#             "weather_code": weather["weather_code"],
#             "weather_text": weather["weather_text"],
#             "source": weather["source"],
#         })

#     sun = fetch_sunrise_sunset(date_str, lat, lon)
#     if sun:
#         insert_or_replace("sun_info", {
#             "date": date_str, "lat": lat, "lon": lon,
#             "sunrise_utc": sun["sunrise_utc"],
#             "sunset_utc": sun["sunset_utc"],
#             "source": sun["source"],
#         })

#     return {
#         "date": date_str,
#         "location": {"lat": lat, "lon": lon},
#         "weather": weather,
#         "sun": sun
#     }


# # -----------------------------
# # Diary read helpers
# # -----------------------------
# def query_diaries(date_from: Optional[str] = None,
#                   date_to: Optional[str] = None,
#                   keyword: str = ""):
#     con = sqlite3.connect(DB_PATH)
#     con.row_factory = sqlite3.Row
#     cur = con.cursor()
#     q = "SELECT id, date, title, body, location, tags, created_at FROM diary_entries WHERE 1=1"
#     params = []
#     if date_from:
#         q += " AND date >= ?"; params.append(date_from)
#     if date_to:
#         q += " AND date <= ?"; params.append(date_to)
#     if keyword.strip():
#         q += " AND (title LIKE ? OR body LIKE ? OR location LIKE ? OR tags LIKE ?)"
#         like = f"%{keyword}%"
#         params += [like, like, like, like]
#     q += " ORDER BY date DESC, id DESC LIMIT 200"
#     cur.execute(q, params)
#     rows = cur.fetchall()
#     con.close()
#     return rows


# def seed_demo_entries():
#     """ãƒ‡ãƒ¢ç¢ºèªç”¨ã®åˆæœŸãƒ‡ãƒ¼ã‚¿ã€‚å¿…è¦ãªã¨ãã ã‘å‘¼ã³å‡ºã—ã¦ãã ã•ã„ã€‚"""
#     con = sqlite3.connect(DB_PATH)
#     cur = con.cursor()
#     cur.execute("SELECT COUNT(*) FROM diary_entries;")
#     n = cur.fetchone()[0]
#     if n == 0:
#         rows = [
#             ("2025-09-22", "æ—¥æ›œã®æ•£æ­©", "åˆå‰ä¸­ã«è¿‘æ‰€ã‚’ã¶ã‚‰ã¶ã‚‰æ­©ã„ãŸã€‚ã‚«ãƒ•ã‚§ã§èª­æ›¸ã€‚", "å¯Œå±±å¸‚", "æ•£æ­©,èª­æ›¸"),
#             ("2025-09-23", "é›¨ã§åœ¨å®…", "ä¸€æ—¥é›¨ã€‚å®¶ã§æƒé™¤ã¨æ–™ç†ã€‚å¤œã¯æ˜ ç”»ã‚’è¦³ãŸã€‚", "å¯Œå±±å¸‚", "åœ¨å®…,é›¨,æ˜ ç”»"),
#             ("2025-09-24", "ç ”ç©¶ã®æ—¥", "å¤§å­¦ã§å®Ÿé¨“ã€‚å¤•æ–¹ã«å‹äººã¨ãƒ©ãƒ¼ãƒ¡ãƒ³ã€‚", "å¯Œå±±å¸‚", "ç ”ç©¶,é£Ÿäº‹")
#         ]
#         cur.executemany(
#             "INSERT INTO diary_entries(date,title,body,location,tags) VALUES (?,?,?,?,?)", rows
#         )
#         con.commit()
#     con.close()


# # -----------------------------
# # CLI
# # -----------------------------
# def parse_args():
#     p = argparse.ArgumentParser(description="Diary interpolation: free-API ingestion")
#     p.add_argument("--cli", action="store_true", help="CLIãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œï¼ˆUIã¯ç„¡åŠ¹ï¼‰")
#     p.add_argument("--date", help="YYYY-MM-DD")
#     p.add_argument("--place", help="åœ°åï¼ˆä¾‹: å¯Œå±±å¸‚ï¼‰")
#     p.add_argument("--lat", type=float, help="ç·¯åº¦")
#     p.add_argument("--lon", type=float, help="çµŒåº¦")
#     return p.parse_args()


# def run_cli(args):
#     # date validation
#     if not args.date:
#         raise SystemExit("ERROR: --date ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
#     try:
#         _ = dt.date.fromisoformat(args.date)
#     except ValueError:
#         raise SystemExit("ERROR: --date ã¯ YYYY-MM-DD å½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")

#     # resolve location
#     if args.place:
#         geo = geocode_place(args.place)
#         if not geo:
#             raise SystemExit("ERROR: åœ°åã‚’ç·¯åº¦çµŒåº¦ã«è§£æ±ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®è¡¨è¨˜ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚")
#         lat, lon, resolved = geo
#         print(f"[INFO] Resolved place: {resolved} -> lat={lat}, lon={lon}")
#     else:
#         if args.lat is None or args.lon is None:
#             raise SystemExit("ERROR: --place ã‹ --lat/--lon ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
#         lat, lon = args.lat, args.lon
#         print(f"[INFO] Using coordinates: lat={lat}, lon={lon}")

#     record = enrich_and_store(args.date, lat, lon)
#     print(json.dumps(record, ensure_ascii=False, indent=2))


# # -----------------------------
# # UI (Streamlit)
# # -----------------------------
# def render_ui():
#     import streamlit as st

#     st.set_page_config(page_title="æ—¥è¨˜ã®è£œé–“ - ãƒ‡ãƒ¼ã‚¿å–å¾—/é–²è¦§", page_icon="ğŸ“’", layout="centered")
#     st.title("ğŸ“’ æ—¥è¨˜ã®è£œé–“ï¼ˆPoCï¼‰")

#     ensure_db()
#     # ãƒ‡ãƒ¢ç¢ºèªã®ã¨ãã ã‘æœ‰åŠ¹åŒ–
#     # seed_demo_entries()

#     tab_fetch, tab_view = st.tabs(["ğŸ›  ãƒ‡ãƒ¼ã‚¿å–å¾—", "ğŸ“— æ—¥è¨˜ã‚’è¦‹ã‚‹"])

#     # ------------ ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¿ãƒ– ------------
#     with tab_fetch:
#         with st.form("fetch_form", clear_on_submit=False):
#             d = st.date_input("æ—¥ä»˜", value=dt.date.today())
#             place = st.text_input("å ´æ‰€ï¼ˆå¸‚åŒºç”ºæ‘åï¼ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯åï¼‰", value="å¯Œå±±å¸‚")
#             lat = st.text_input("ç·¯åº¦ï¼ˆæœªå…¥åŠ›ãªã‚‰åœ°åã‚’ä½¿ç”¨ï¼‰", value="")
#             lon = st.text_input("çµŒåº¦ï¼ˆæœªå…¥åŠ›ãªã‚‰åœ°åã‚’ä½¿ç”¨ï¼‰", value="")
#             submitted = st.form_submit_button("å–å¾—ã—ã¦ä¿å­˜ã™ã‚‹")

#         if submitted:
#             date_str = d.isoformat()

#             # resolve location
#             lat_f = lon_f = None
#             if lat.strip() and lon.strip():
#                 try:
#                     lat_f = float(lat); lon_f = float(lon)
#                 except ValueError:
#                     st.error("ç·¯åº¦ãƒ»çµŒåº¦ã¯æ•°å€¤ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
#                     st.stop()
#             else:
#                 geo = geocode_place(place)
#                 if not geo:
#                     st.error("åœ°åã‚’ç·¯åº¦çµŒåº¦ã«è§£æ±ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®è¡¨è¨˜ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚")
#                     st.stop()
#                 lat_f, lon_f, resolved = geo
#                 st.info(f"è§£æ±º: {resolved} â†’ lat={lat_f}, lon={lon_f}")

#             with st.spinner("å–å¾—ä¸­â€¦"):
#                 rec = enrich_and_store(date_str, lat_f, lon_f)

#             st.success("å–å¾—ãƒ»ä¿å­˜ã—ã¾ã—ãŸï¼ˆ./data/diary_enriched.sqliteï¼‰")
#             st.code(json.dumps(rec, ensure_ascii=False, indent=2), language="json")

#         if st.checkbox("DBã®ä¸­èº«ã‚’å°‘ã—è¦‹ã‚‹ï¼ˆweather_daily å…ˆé ­10ä»¶ï¼‰"):
#             import pandas as pd  # ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¡¨ç¤ºã§ã®ã¿ä½¿ç”¨
#             con = sqlite3.connect(DB_PATH)
#             df = pd.read_sql_query("SELECT * FROM weather_daily ORDER BY date DESC LIMIT 10;", con)
#             st.dataframe(df)
#             con.close()

#     # ------------ æ—¥è¨˜é–²è¦§ã‚¿ãƒ– ------------
#     with tab_view:
#         st.subheader("æ—¥è¨˜ä¸€è¦§ï¼ˆé–²è¦§ç”¨ï¼‰")

#         col1, col2 = st.columns(2)
#         with col1:
#             df = st.date_input("é–‹å§‹æ—¥", value=None)
#         with col2:
#             dt_ = st.date_input("çµ‚äº†æ—¥", value=None)

#         kw = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚¿ã‚¤ãƒˆãƒ«/æœ¬æ–‡/å ´æ‰€/ã‚¿ã‚°ã‹ã‚‰æ¤œç´¢ï¼‰", placeholder="ä¾‹: ç ”ç©¶, é›¨, ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°")

#         # ãƒ•ã‚£ãƒ«ã‚¿å®Ÿè¡Œ
#         date_from = df.isoformat() if df else None
#         date_to = dt_.isoformat() if dt_ else None
#         rows = query_diaries(date_from, date_to, kw)

#         if not rows:
#             st.info("æ¡ä»¶ã«åˆã†æ—¥è¨˜ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ«ã‚¿ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚")
#         else:
#             for r in rows:
#                 with st.expander(f"{r['date']}  â€”  {r['title'] or '(ç„¡é¡Œ)'}  [{r['location'] or '-'}]"):
#                     st.markdown(f"**ã‚¿ã‚°**: {r['tags'] or '-'}")
#                     st.markdown("---")
#                     st.markdown(r["body"])
#                     st.caption(f"ID: {r['id']} / ä½œæˆ: {r['created_at']}")

#             # ä¸€è¦§CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
#             if st.button("ã“ã®ä¸€è¦§ã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
#                 import pandas as pd, io  # ã‚ªãƒ—ã‚·ãƒ§ãƒ³æ™‚ã®ã¿
#                 df_csv = pd.DataFrame([dict(row) for row in rows])
#                 csv = df_csv.to_csv(index=False)
#                 st.download_button("CSVã‚’ä¿å­˜", data=csv, file_name="diaries.csv", mime="text/csv")


# if __name__ == "__main__":
#     args = parse_args()
#     if args.cli:
#         run_cli(args)
#     else:
#         render_ui()
